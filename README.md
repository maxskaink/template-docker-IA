# Microservicio de Clasificaci√≥n de Texto con IA

## üöÄ Descripci√≥n

Esta es una plantilla de microservicio RESTful construida con **FastAPI** para servir modelos de inteligencia artificial. El servicio implementa un clasificador de texto que determina el sentimiento (positivo/negativo) de textos en espa√±ol.

### Caracter√≠sticas principales:

- ‚öôÔ∏è **Framework**: FastAPI con tipado est√°tico
- ü§ñ **Modelo IA**: Clasificaci√≥n de texto con TF-IDF + Logistic Regression
- üê≥ **Contenerizaci√≥n**: Docker y Docker Compose
- üìã **Logging**: Sistema completo con Loguru
- ‚úÖ **Validaci√≥n**: Manejo robusto de errores y validaci√≥n de entradas
- üîÑ **Optimizaci√≥n**: Carga del modelo una sola vez al iniciar

## üìú Estructura del Proyecto

```
plantilla_docker/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # Punto de entrada de FastAPI
‚îÇ   ‚îî‚îÄ‚îÄ model.py          # L√≥gica del modelo de IA
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias de Python
‚îú‚îÄ‚îÄ Dockerfile            # Configuraci√≥n del contenedor
‚îú‚îÄ‚îÄ docker-compose.yml    # Orquestaci√≥n de servicios
‚îú‚îÄ‚îÄ .dockerignore         # Archivos excluidos del build
‚îî‚îÄ‚îÄ README.md             # Esta documentaci√≥n
```

## üöÄ Inicio R√°pido

### Opci√≥n 1: Con Docker Compose (Recomendado)

```bash
# Clonar o descargar la plantilla
cd plantilla_docker

# Construir y ejecutar el servicio
docker-compose up --build

# El servicio estar√° disponible en http://localhost:8000
```

### Opci√≥n 2: Con Docker

```bash
# Construir la imagen
docker build -t ai-microservice .

# Ejecutar el contenedor
docker run -p 8000:8000 ai-microservice
```

### Opci√≥n 3: Desarrollo Local

```bash
# Instalar dependencias
pip install -r requirements.txt

# Ejecutar el servidor de desarrollo
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## üìö Endpoints de la API

### Documentaci√≥n Interactiva
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints Principales

#### 1. Health Check
```http
GET /health
```

**Respuesta:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_info": {
    "status": "loaded",
    "model_type": "TF-IDF + Logistic Regression",
    "features": 1000,
    "classes": ["negativo", "positivo"]
  }
}
```

#### 2. Predicci√≥n Individual
```http
POST /predict
Content-Type: application/json

{
  "text": "Este producto es excelente, lo recomiendo mucho"
}
```

**Respuesta:**
```json
{
  "text": "Este producto es excelente, lo recomiendo mucho",
  "prediction": "positivo",
  "confidence": 0.887,
  "probabilities": {
    "negativo": 0.113,
    "positivo": 0.887
  }
}
```

#### 3. Predicci√≥n en Lote
```http
POST /predict/batch
Content-Type: application/json

{
  "texts": [
    "Me encanta este producto",
    "Muy malo, no lo recomiendo",
    "Excelente calidad"
  ]
}
```

**Respuesta:**
```json
{
  "results": [
    {
      "text": "Me encanta este producto",
      "prediction": "positivo",
      "confidence": 0.923,
      "probabilities": {"negativo": 0.077, "positivo": 0.923}
    },
    {
      "text": "Muy malo, no lo recomiendo",
      "prediction": "negativo",
      "confidence": 0.856,
      "probabilities": {"negativo": 0.856, "positivo": 0.144}
    }
  ],
  "total_processed": 3
}
```

#### 4. Informaci√≥n del Modelo
```http
GET /model/info
```

## üîç Ejemplos de Uso

### Con cURL

```bash
# Health check
curl -X GET "http://localhost:8000/health"

# Predicci√≥n individual
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"text": "Este servicio funciona perfectamente"}'

# Predicci√≥n en lote
curl -X POST "http://localhost:8000/predict/batch" \
     -H "Content-Type: application/json" \
     -d '{"texts": ["Excelente", "Terrible", "Regular"]}'
```

### Con Python

```python
import requests

# URL base del servicio
base_url = "http://localhost:8000"

# Predicci√≥n individual
response = requests.post(
    f"{base_url}/predict",
    json={"text": "Este producto es incre√≠ble"}
)
print(response.json())

# Predicci√≥n en lote
response = requests.post(
    f"{base_url}/predict/batch",
    json={
        "texts": [
            "Me gusta mucho",
            "No me gusta nada",
            "Est√° bien"
        ]
    }
)
print(response.json())
```

### Con JavaScript/Fetch

```javascript
// Predicci√≥n individual
fetch('http://localhost:8000/predict', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    text: 'Este servicio es fant√°stico'
  })
})
.then(response => response.json())
.then(data => console.log(data));
```

## üîß Personalizaci√≥n

### Cambiar el Modelo de IA

Para usar tu propio modelo, modifica el archivo `app/model.py`:

1. **Reemplaza la clase `TextClassificationModel`** con tu implementaci√≥n
2. **Mantiene la interfaz p√∫blica**:
   - `load_model()`: Cargar modelo
   - `predict(text)`: Predicci√≥n individual
   - `predict_batch(texts)`: Predicci√≥n en lote
   - `get_model_info()`: Informaci√≥n del modelo

### Ejemplo con Hugging Face

```python
from transformers import pipeline

class HuggingFaceModel:
    def __init__(self):
        self.model = None
        self.is_loaded = False
    
    def load_model(self):
        try:
            self.model = pipeline(
                "sentiment-analysis",
                model="nlptown/bert-base-multilingual-uncased-sentiment"
            )
            self.is_loaded = True
            return True
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            return False
    
    def predict(self, text):
        result = self.model(text)[0]
        return {
            "text": text,
            "prediction": result["label"],
            "confidence": result["score"],
            "probabilities": {result["label"]: result["score"]}
        }
```

### Variables de Entorno

Puedes configurar el servicio con variables de entorno:

```bash
# Puerto del servicio
export PORT=8000

# Nivel de logging
export LOG_LEVEL=INFO

# Ruta del modelo
export MODEL_PATH=/app/models/custom_model.joblib
```

## üìã Logs y Monitoreo

### Estructura de Logs

- **Consola**: Logs informativos con formato simple
- **Archivo**: Logs detallados en `logs/app.log` con rotaci√≥n autom√°tica

### Ver Logs del Contenedor

```bash
# Logs en tiempo real
docker-compose logs -f ai-microservice

# √öltimas 100 l√≠neas
docker-compose logs --tail=100 ai-microservice
```

### M√©tricas de Health Check

El endpoint `/health` proporciona:
- Estado del servicio
- Estado de carga del modelo
- Informaci√≥n del modelo actual
- M√©tricas de rendimiento

## üö™ Despliegue en Producci√≥n

### Consideraciones de Seguridad

1. **CORS**: Modifica `allow_origins` en `main.py` para especificar dominios permitidos
2. **HTTPS**: Usa un proxy reverso (Nginx, Traefik) para SSL/TLS
3. **Secrets**: No hardcodees claves API o secretos

### Optimizaciones

1. **Multi-stage build**: Optimiza el Dockerfile para im√°genes m√°s peque√±as
2. **Resource limits**: Configura l√≠mites de CPU/memoria en docker-compose.yml
3. **Caching**: Implementa caching para predicciones frecuentes

### Ejemplo con Nginx

```nginx
server {
    listen 80;
    server_name tu-dominio.com;
    
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## üìà Escalabilidad

### Horizontal Scaling

```yaml
# docker-compose.yml para m√∫ltiples instancias
services:
  ai-microservice:
    # ... configuraci√≥n existente ...
    deploy:
      replicas: 3
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - ai-microservice
```

### Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-microservice
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-microservice
  template:
    metadata:
      labels:
        app: ai-microservice
    spec:
      containers:
      - name: ai-microservice
        image: ai-microservice:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
```

## üõ†Ô∏è Troubleshooting

### Problemas Comunes

1. **Modelo no carga**:
   ```bash
   # Verificar logs
   docker-compose logs ai-microservice | grep -i error
   
   # Verificar salud del servicio
   curl http://localhost:8000/health
   ```

2. **Puerto ocupado**:
   ```bash
   # Cambiar puerto en docker-compose.yml
   ports:
     - "8001:8000"  # Puerto 8001 en lugar de 8000
   ```

3. **Memoria insuficiente**:
   ```bash
   # Aumentar memoria disponible para Docker
   # O usar un modelo m√°s peque√±o
   ```

### Debugging

```bash
# Entrar al contenedor
docker-compose exec ai-microservice bash

# Verificar estructura de archivos
ls -la /app/

# Verificar logs en tiempo real
tail -f logs/app.log
```

## üìö Recursos Adicionales

- [Documentaci√≥n de FastAPI](https://fastapi.tiangolo.com/)
- [Gu√≠a de Docker](https://docs.docker.com/)
- [Scikit-learn](https://scikit-learn.org/stable/)
- [Transformers de Hugging Face](https://huggingface.co/transformers/)

## ü§ù Contribuci√≥n
¬°Las contribuciones son bienvenidas! Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature
3. Commit tus cambios
4. Env√≠a un Pull Request

## üìú Licencia

Este proyecto es de c√≥digo abierto bajo la Licencia MIT.

---

¬øüöÄ ¬øListo para servir tu modelo de IA? ¬°Clona esta plantilla y personaliza seg√∫n tus necesidades!

